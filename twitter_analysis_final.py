# -*- coding: utf-8 -*-
"""Twitter_analysis_final.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1AuzA9VfAuXT3xVw3njWTOckdnLljoiyU

## Loading packages
"""

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# %matplotlib inline
# #data manipulation
# import pandas as pd
# import numpy as np
# import collections
# import regex as re 
# import random
# #data wordcloud
# from textblob import TextBlob
# from textblob import Word
# from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator
# #import visualization
# import matplotlib.pyplot as plt
# import io
# import base64
# #networks
# import networkx as nx
# !pip install python-louvain
# from community import community_louvain
# #nltk
# import nltk
# from nltk.tokenize import word_tokenize
# from nltk.tokenize import wordpunct_tokenize
# #pickle
# import pickle

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# !pip install plotly --upgrade
# from plotly import __version__
# print (__version__)
# import plotly.express as px
# from plotly.subplots import make_subplots
# import plotly.graph_objs as go
# import plotly as py
# #import plotly.figure_factory as ff
# from plotly import tools
# #import cufflinks as cf
# from plotly.offline import iplot, init_notebook_mode
# # Display all cell outputs
# from IPython.core.interactiveshell import InteractiveShell

nltk.download('stopwords')
 nltk.download('rslp')

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# import plotly.io as pio
# pio.templates.default = "plotly_white"
# #pio.renderers.default = 'colab'

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# !pip install dash
# import dash
# !pip install jupyter-dash
# from jupyter_dash import JupyterDash
# !pip install dash_bootstrap_components
# import dash_bootstrap_components as dbc
# import dash_core_components as dcc
# import dash_html_components as html
# from dash.dependencies import Input, Output, State

"""## Loading Dataset"""

url = 'https://raw.githubusercontent.com/Fmarcelneves/Twitter_Network_Analytics/main/twitter_data_eleicoes-2020.csv'

twitter_dt = pd.read_csv(url)

twitter_dt.head(5)

"""## Exploratory analysis """

twitter_dt.shape

twitter_dt.describe()

twitter_dt_dummy = twitter_dt.copy()
twitter_dt_dummy.loc[twitter_dt_dummy['Retweets'] != 0, 'Retweets'] = "Presente"
twitter_dt_dummy.loc[twitter_dt_dummy['Retweets'] == 0, 'Retweets'] = "Ausente"
twitter_dt_dummy.loc[twitter_dt_dummy['Curtidas'] != 0, 'Curtidas'] = "Presente"
twitter_dt_dummy.loc[twitter_dt_dummy['Curtidas'] == 0, 'Curtidas'] = "Ausente"
twitter_dt_dummy = twitter_dt_dummy[['Retweets', 'Curtidas', 'Usuário_verificado', 'Mídia']]

pd.options.plotting.backend = "plotly"

fig1 = twitter_dt_dummy['Retweets'].value_counts().plot.bar(color = ['Ausente', 'Presente'], title = "Retweets", labels={"value": "Frequências", "index": "", 'color': ''})
fig1.show()

fig2 = twitter_dt_dummy['Curtidas'].value_counts().reindex(["Ausente", "Presente"]).plot.bar(color = ['Ausente', 'Presente'], title = "Curtidas no tweet", 
      labels={"value": "Frequências", "index": "", 'color': ''})
fig2.show()

fig3 = twitter_dt['Mídia'].value_counts().plot.bar(color = ['Ausente', 'Presente'],  title = "Mídia", 
                                                   labels={"value": "Frequências", "index": "", 'color': ''})
fig3.show()

index_names= twitter_dt[ (twitter_dt['Retweets'] <= 10)].index
twitter_dt_rt = twitter_dt.drop(index_names) 
fig4 = twitter_dt_rt['Retweets'].plot.hist(title = "Frequência de retweets", bins= 80, 
                                           labels={"count": "Frequências", "value": "Retweets", 'variable': ''})
fig4.show()

index_names= twitter_dt[(twitter_dt['Curtidas'] <= 10)].index
twitter_dt_ct = twitter_dt.drop(index_names) 
fig5 = twitter_dt_ct['Curtidas'].plot.hist(title = "Frequência de curtidas", bins = 80,
labels={"count": "Frequências", "value": "Curtidas", "variable": ""})
fig5

index_names= twitter_dt[ (twitter_dt['Seguidores'] <= 30)].index
twitter_dt_sg = twitter_dt.drop(index_names) 
fig6 = twitter_dt_sg['Seguidores'].plot.hist(title = "Frequência de seguidores", bins = 100000, 
labels={"count": "Frequências", "value": "Seguidores", "variable":""})
fig6.show()

fig7 = px.scatter(twitter_dt, x="Retweets", color="Usuário_verificado", marginal_x ="box", facet_col="Mídia", title = "Número de retweets na presença e ausência de mídia", 
labels={"index": "Frequências", "Usuário_verificado": "Usuário verificado"})
fig7.show()

twitter_dt_tempo = twitter_dt.groupby([twitter_dt['Tempo'].astype('datetime64[ns]').dt.to_period('1d')]).count()
fig8=px.bar(twitter_dt_tempo, x=twitter_dt_tempo.index.astype(str) , y="Id", labels={"Id": "Frequências", "x": "Tempo"},
       title="Frequências dos tweets ao longo do tempo", color = twitter_dt_tempo['Id'])
fig8.show()

correlation = twitter_dt[["Seguidores", "Retweets", "Curtidas"]].corr()
fig9 = px.imshow(correlation, labels=dict(color="Correlação"), title="Correlação entre as variáveis numéricas")
#fig.update_layout(width=800, height=800)
fig9.show()

net1 = pd.DataFrame(twitter_dt['Nome_do_usuário'])
net2 = pd.DataFrame(twitter_dt['Hashtags'].str.split().values.tolist())
net_rtc = pd.DataFrame(twitter_dt[['Nome_do_usuário', 'Retweets']])

net_all = pd.concat([net1, net2.reindex(net1.index)], axis=1)
net_all = net_all.melt(id_vars="Nome_do_usuário")

net_all['value'] = net_all['value'].str.replace('[','').str.replace(']','').str.replace(',','')
net_all['value'].replace('', np.nan, inplace=True)
net_all.dropna(inplace=True)

net_all.drop(['variable'], axis=1, inplace=True)

net_all_word = net_all

net_all["count"] = 1
net_all = net_all.groupby(["Nome_do_usuário", "value"])["count"].count().reset_index()

pd.options.plotting.backend = "plotly"

net_all_word

fig10 = twitter_dt["Nome_do_usuário"].value_counts().nlargest(10).plot(kind='barh',title="Contas do twitter mais ativas", 
labels={"value": "Frequências", "index": "Contas do Twitter", 'color': 'Frequência'}, color = twitter_dt["Nome_do_usuário"].value_counts().nlargest(10))
fig10.update_layout(showlegend=False)

followers = twitter_dt.sort_values(by = "Seguidores", ascending = False)
user_most_followers = twitter_dt.groupby("Nome_do_usuário").max().sort_values(by = "Seguidores", ascending = False)
user_most_followers["Nome_do_usuário"] = user_most_followers.index
user_most_followers.reset_index(inplace = True, drop = True)
fig11=px.bar(user_most_followers[:10], x = "Nome_do_usuário", y = "Seguidores", color = "Usuário_verificado", title="Usuários com maior número de seguidores", 
             labels={"Seguidores": "Seguidores", "Nome_do_usuário": "Usuários", 'Usuário_verificado': 'Usuário verificado'})
fig11.show()

retweeted = twitter_dt.sort_values(by = "Retweets", ascending = False)
user_most_retweeted = twitter_dt.groupby("Nome_do_usuário").max().sort_values(by = "Retweets", ascending = False)
user_most_retweeted['Nome_do_usuário'] = user_most_retweeted.index
user_most_retweeted.reset_index(inplace = True, drop = True)
fig12 = px.bar(user_most_retweeted[:10], x = 'Nome_do_usuário', y = "Retweets", color = "Usuário_verificado", title="Usuários com maior número de retweets", 
               labels={"Nome_do_usuário": "Nome do usuário", 'Usuário_verificado': 'Usuário verificado'})
fig12.update_layout(xaxis_categoryorder = 'total descending')
fig12.show()

"""## Wordcloud"""

url = 'https://raw.githubusercontent.com/Fmarcelneves/Twitter_Network_Analytics/main/stopwords.txt'

stopwords= set(STOPWORDS)
#Adicionando a lista stopwords em português
new_words = []
with open('stopwords.txt', 'r') as f:
    [new_words.append(word) for line in f for word in line.split()]

new_stopwords = stopwords.union(new_words + ["#eleicoes2020"])

"""
#Adicionando a lista stopwords em português"""

import io
import base64

Hashtag_Combined = " ".join(twitter_dt['Hashtags'].dropna())
text = Hashtag_Combined.replace("eleicoes2020", "").replace("eleicoe", "").replace("election2020", "elections2020").replace("smunicipais", "eleicoesmunicipais").replace("samaericanas2020", "eleicoesamericanas2020").replace("snoseua", "eleicoesnoseua").replace("seua", "eleicoeseua")

text

#Word Cloud
fig14 = WordCloud(background_color="black", stopwords=STOPWORDS, width=1800, height=1500, collocations=False)
text = Hashtag_Combined.replace("eleicoes2020", "").replace("eleicoe", "").replace("election2020", "elections2020").replace("smunicipais", "eleicoesmunicipais").replace("samaericanas2020", "eleicoesamericanas2020").replace("snoseua", "eleicoesnoseua").replace("seua", "eleicoeseua")
fig14.generate(text)
plt.figure(figsize=(12,12))
plt.axis("off")
plt.imshow(fig14, interpolation='bilinear');

teste = str(list(net_all_word['value']))

hash_freq = net_all_word[~net_all_word.value.str.contains("#eleicoes2020")]

wordcloud = pd.DataFrame(hash_freq['value'].value_counts().nlargest(200))

text = list(wordcloud.index)
frequency = list(wordcloud.value)

words = text
frequency = frequency

lower, upper = 5, 45
frequency = [((x - min(frequency)) / (max(frequency) - min(frequency))) * (upper - lower) + lower for x in frequency]

lenth = len(words)

colors = [py.colors.DEFAULT_PLOTLY_COLORS[random.randrange(1, 10)] for i in range(lenth)]

py.colors

data = go.Scatter(x=random.choices(range(lenth), k=lenth),  
 y=random.choices(range(lenth), k=lenth),
mode='text',
text=words,
hovertext=['{0} {1}'.format(w, f) for w, f in zip(words, frequency)],
hoverinfo='text',
textfont={'size': frequency, 'color': colors})
layout = go.Layout({'xaxis': {'showgrid': False, 'showticklabels': False, 'zeroline': False},
                    'yaxis': {'showgrid': False, 'showticklabels': False, 'zeroline': False}}, 
                   margin=dict(l=25, r=25, t=25, b=25), title="Wordcloud das Hashtags presentes nos tweets")

fig13 = go.Figure(data=[data], layout=layout)
fig13.show()

hash_freq = net_all_word[~net_all_word.value.str.contains("#eleicoes2020")]
fig14=hash_freq['value'].value_counts().nlargest(15).plot(kind='bar', title="Frequência das Hashtags presentes nos tweets", labels={"_value": "Frequências", "index": "Hashtags", 'color': 'Frequência'}, color = hash_freq['value'].value_counts().nlargest(15))
fig14.update_layout(showlegend=False)



"""## Noise Removal Remove urls from text (http(s), www)

"""

def _remove_url(data):
    ls = []
    words = ''
    regexp1 = re.compile('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\(\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')
    regexp2 = re.compile('www?.(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\(\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')
    
    for line in data:
        urls = regexp1.findall(line)

        for u in urls:
            line = line.replace(u, ' ')

        urls = regexp2.findall(line)

        for u in urls:
            line = line.replace(u, ' ')
            
        ls.append(line)
    return ls

sa_twitter_dt = _remove_url(twitter_dt['Tweet'])

def _remove_regex(data, regex_pattern):
    ls = []
    words = ''
    
    for line in data:
        matches = re.finditer(regex_pattern, line)
        
        for m in matches: 
            line = re.sub(m.group().strip(), '', line)

        ls.append(line)

    return ls

# notations
regex_pattern = '@[\w]*'
sa_twitter_dt = _remove_regex(sa_twitter_dt, regex_pattern)

# check data
for i in range(0, 5):
    print(sa_twitter_dt[i])

# rt
regex_pattern = 'RT[\w]*'
sa_twitter_dt = _remove_regex(sa_twitter_dt, regex_pattern)

# hashtags
regex_pattern = '#[\w]*'
sa_twitter_dt = _remove_regex(sa_twitter_dt, regex_pattern)

# notations
regex_pattern = '@[\w]*'
sa_twitter_dt = _remove_regex(sa_twitter_dt, regex_pattern)

emoji_pattern = re.compile("["
                           u"\U0001F600-\U0001F64F"  # emoticons
                           u"\U0001F300-\U0001F5FF"  # symbols & pictographs
                           u"\U0001F680-\U0001F6FF"  # transport & map symbols
                           u"\U0001F1E0-\U0001F1FF"  # flags (iOS)
                           u"\U00002702-\U000027B0"
                           u"\U000024C2-\U0001F251"
                           "]+", flags=re.UNICODE)

# emoji pattern 
regex_pattern = emoji_pattern 
sa_twitter_dt = _remove_regex(sa_twitter_dt, regex_pattern)

# check data
for i in range(0, 5):
    print(sa_twitter_dt[i])

def _replace_emoticons(data, emoticon_list):
    ls = []

    for line in data:
        for exp in emoticon_list:
            line = line.replace(exp, emoticon_list[exp])

        ls.append(line)

    return ls

emoticon_list = {':))': 'emoticon_positivo', ':)': 'emoticon_positivo', ':D': 'emoticon_positivo', ':(': 'emoticon_negativo', ':((': 'emoticon_negativo', '8)': 'emoticon_neutro'}

sa_twitter_dt = _replace_emoticons(sa_twitter_dt, emoticon_list)

# check data
for i in range(0, 5):
    print(sa_twitter_dt[i])

def _tokenize_text(data):
    ls = []

    for line in data:
        tokens = wordpunct_tokenize(line)
        ls.append(tokens)

    return ls

sa_twitter_dt_tokens = _tokenize_text(sa_twitter_dt)

# check data
for i in range(0, 5):
    print(sa_twitter_dt_tokens [i])

"""### Object Standardization"""

def _apply_standardization(tokens, std_list):
    ls = []

    for tk_line in tokens:
        new_tokens = []
        
        for word in tk_line:
            if word.lower() in std_list:
                word = std_list[word.lower()]
                
            new_tokens.append(word) 
            
        ls.append(new_tokens)

    return ls

std_list = {'eh': 'é', 'vc': 'você', 'vcs': 'vocês','tb': 'também', 'tbm': 'também', 'obg': 'obrigado', 'gnt': 'gente', 'q': 'que', 'n': 'não', 'cmg': 'comigo', 'p': 'para', 'ta': 'está', 'to': 'estou', 'vdd': 'verdade'}

sa_twitter_dt_tokens = _apply_standardization(sa_twitter_dt_tokens, std_list)

sa_twitter_dt_tokens[0]

def _remove_stopwords(tokens, stopword_list):
    ls = []

    for tk_line in tokens:
        new_tokens = []
        
        for word in tk_line:
            if word.lower() not in stopword_list:
                new_tokens.append(word) 
            
        ls.append(new_tokens)
        
    return ls

stopword_list = []

# get nltk portuguese stopwords
nltk_stopwords = nltk.corpus.stopwords.words('portuguese')

# You can also add stopwords manually instead of loading from the database. Generally, we add stopwords that belong to this context.
stopword_list.append('é')
stopword_list.append('vou')
stopword_list.append('que')
stopword_list.append('tão')
stopword_list.append('...')
stopword_list.append('«')
stopword_list.append('➔')
stopword_list.append('|')
stopword_list.append('//')
stopword_list.append('/')
stopword_list.append(':')
stopword_list.append('-')
stopword_list.append(';')
stopword_list.append('»')
stopword_list.append('…')
stopword_list.append(')')
stopword_list.append('(')
stopword_list.append(']')
stopword_list.append('[')
stopword_list.append('((')
stopword_list.append('))')
stopword_list.append('emoticon_positivo') 
stopword_list.append('emoticon_negativo')
stopword_list.append('emoticon_neutro')
stopword_list.append('uai') # 'expression from the mineiros (MG/Brazil)'

stopword_list.extend(nltk_stopwords)

stopword_list = list(set(stopword_list))

sa_twitter_dt_tokens = _remove_stopwords(sa_twitter_dt_tokens, stopword_list)

# check data
for i in range(0, 5):
    print(sa_twitter_dt_tokens[i])

"""### Stemming (dimensionality reduction)"""

def _apply_stemmer(tokens):
    ls = []
    stemmer = nltk.stem.RSLPStemmer()

    for tk_line in tokens:
        new_tokens = []
        
        for word in tk_line:
            word = str(stemmer.stem(word))
            new_tokens.append(word) 
            
        ls.append(new_tokens)
        
    return ls

sa_twitter_dt_tokens = _apply_stemmer(sa_twitter_dt_tokens)

# check data
for i in range(0, 5):
    print(sa_twitter_dt_tokens[i])

"""### Untokenize text (transform tokenized text into string list)"""

def _untokenize_text(tokens):
    ls = []

    for tk_line in tokens:
        new_line = ''
        
        for word in tk_line:
            new_line += word + ' '
            
        ls.append(new_line)
        
    return ls

sa_twitter_dt = _untokenize_text(sa_twitter_dt_tokens)

# check data
for i in range(0, 5):
    print(sa_twitter_dt[i])

"""### Incorporar o modelo de análise de sentimentos """

model = "https://github.com/Fmarcelneves/Twitter_Network_Analytics/blob/main/LR_model_we"

loaded_model = pickle.load(open('LR_model_we', 'rb'))

labels_sentiments= loaded_model.predict(sa_twitter_dt)

Sentimento_tweet = pd.DataFrame(labels_sentiments, columns = ['Sentimento'])

twitter_dt['Sentimento_tweet'] = Sentimento_tweet

twitter_dt.head()

"""### Análise de sentimentos """

twitter_dt['Sentimento_tweet'].describe()

twitter_dt['Sentimento_tweet'].value_counts()

pd.options.plotting.backend = "plotly"

fig15 = twitter_dt['Sentimento_tweet'].value_counts().plot.bar(color = ['Neutro', 'Positivo', 'Negativo'], title = "Sentimentos presentes nos tweets", 
                                                               labels={"value": "Frequências", "index": "", 'color': ''}, color_discrete_sequence =['#00CC96', '#636EFA', '#EF553B'])
fig15.show()

Positivo = twitter_dt[(twitter_dt['Sentimento_tweet'] == "Positivo")]
Negativo= twitter_dt[(twitter_dt['Sentimento_tweet'] == "Negativo")]
Neutro= twitter_dt[(twitter_dt['Sentimento_tweet'] == "Neutro")]

print(Positivo['Tweet'].sample(5).values)

print(Neutro['Tweet'].sample(5).values)

print(Negativo['Tweet'].sample(5).values)

Positivo.head()

net1_pos = pd.DataFrame(Positivo['Nome_do_usuário'])
net2_pos = pd.DataFrame(Positivo['Menções_de_usuários'].str.split().values.tolist())

net_all_pos = pd.concat([net1_pos, net2_pos.reindex(net1.index)], axis=1)
net_all_pos = net_all_pos.melt(id_vars='Nome_do_usuário')

net_all_pos['value'] = net_all_pos['value'].str.replace('[','').str.replace(']','').str.replace(',','')
net_all_pos['value'].replace('', np.nan, inplace=True)
net_all_pos.dropna(inplace=True)

net_all_pos.drop(['variable'], axis=1, inplace=True)

net_all_word_pos = net_all_pos

teste_pos = str(list(net_all_word_pos['value']))

hash_freq_pos = net_all_word_pos[~net_all_word_pos.value.str.contains("#eleicoes2020")]

wordcloud_pos = pd.DataFrame(hash_freq_pos['value'].value_counts().nlargest(200))

text_pos = list(wordcloud_pos.index)
frequency_pos = list(wordcloud_pos.value)

words = text_pos
frequency = frequency_pos

lower, upper = 5, 45
frequency = [((x - min(frequency)) / (max(frequency) - min(frequency))) * (upper - lower) + lower for x in frequency]

lenth = len(words)

colors = '#1f77b4'

data = go.Scatter(x=random.choices(range(lenth), k=lenth),  
 y=random.choices(range(lenth), k=lenth),
mode='text',
text=words,
hovertext=['{0} {1}'.format(w, f) for w, f in zip(words, frequency)],
hoverinfo='text',
textfont={'size': frequency, 'color': colors})
layout = go.Layout({'xaxis': {'showgrid': False, 'showticklabels': False, 'zeroline': False},
                    'yaxis': {'showgrid': False, 'showticklabels': False, 'zeroline': False}}, 
                   margin=dict(l=25, r=25, t=25, b=25), title="Wordcloud das Menções em tweets positivos")

fig16 = go.Figure(data=[data], layout=layout)
fig16.show()

net1_neg = pd.DataFrame(Negativo['Nome_do_usuário'])
net2_neg = pd.DataFrame(Negativo['Menções_de_usuários'].str.split().values.tolist())

net_all_neg = pd.concat([net1_neg, net2_neg.reindex(net1.index)], axis=1)
net_all_neg = net_all_neg.melt(id_vars='Nome_do_usuário')

net_all_neg['value'] = net_all_neg['value'].str.replace('[','').str.replace(']','').str.replace(',','')
net_all_neg['value'].replace('', np.nan, inplace=True)
net_all_neg.dropna(inplace=True)

net_all_neg.drop(['variable'], axis=1, inplace=True)

net_all_word_neg = net_all_neg

hash_freq_neg = net_all_word_neg[~net_all_word_neg.value.str.contains("#eleicoes2020")]

wordcloud_neg = pd.DataFrame(hash_freq_neg['value'].value_counts().nlargest(200))

text_neg = list(wordcloud_neg.index)
frequency_neg = list(wordcloud_neg.value)

words = text_neg
frequency = frequency_neg

lower, upper = 5, 45
frequency = [((x - min(frequency)) / (max(frequency) - min(frequency))) * (upper - lower) + lower for x in frequency]

lenth = len(words)

colors = '#d62728'

data = go.Scatter(x=random.choices(range(lenth), k=lenth),  
 y=random.choices(range(lenth), k=lenth),
mode='text',
text=words,
hovertext=['{0} {1}'.format(w, f) for w, f in zip(words, frequency)],
hoverinfo='text',
textfont={'size': frequency, 'color': colors})
layout = go.Layout({'xaxis': {'showgrid': False, 'showticklabels': False, 'zeroline': False},
                    'yaxis': {'showgrid': False, 'showticklabels': False, 'zeroline': False}}, 
                   margin=dict(l=25, r=25, t=25, b=25), title="Wordcloud das Menções em tweets negativos")

fig17 = go.Figure(data=[data], layout=layout)
fig17.show()

Positivo_count= pd.DataFrame(Positivo['Tempo'].groupby([Positivo['Tempo'].astype('datetime64[ns]').dt.to_period('1d')]).count())
Negativo_count= pd.DataFrame(Negativo['Tempo'].groupby([Negativo['Tempo'].astype('datetime64[ns]').dt.to_period('1d')]).count())
Positivo_count['perc']= Positivo_count['Tempo']/Positivo_count['Tempo'].sum()*100
Negativo_count['perc']= Negativo_count['Tempo']/Negativo_count['Tempo'].sum()*100

data = [go.Scatter(x=Positivo_count.index.astype(str) , y=Positivo_count['perc'], name='Positivo', yaxis='y2'), 
go.Scatter(x=Negativo_count.index.astype(str) , y=Negativo_count['perc'], name='Negativo')]
# settings for the new y axis
y2 = go.layout.YAxis(overlaying='y', side='right')
# adding the second y axis
layout = go.Layout(yaxis2=y2)
fig18 = go.Figure(data=data, layout=go.Layout(yaxis2=y2))
fig18.update_layout(title="Frequências dos tweets positivos e negativos ao longo do tempo", xaxis_title="Tempo", yaxis_title="Frequências(%)")
iplot(fig18)

"""## Redes"""

net_rt = pd.concat([net_rtc, net2.reindex(net_rtc.index)], axis=1)
net_rt = net_rt.melt(id_vars=['Nome_do_usuário', 'Retweets'])

net_rt['value'] = net_rt['value'].str.replace('[','').str.replace(']','').str.replace(',','')
net_rt['value'].replace('', np.nan, inplace=True)
net_rt.dropna(inplace=True)

net_rt.drop(['variable'], axis=1, inplace=True)

net_rt=net_rt[net_rt!=0].dropna()
net_rt = net_rt.groupby(["Nome_do_usuário", "value"])["Retweets"].count().reset_index()

df = net_rt
df.columns = ['source', 'target', 'weight']

df_all = net_rt
df_all.columns = ['source', 'target', 'weight']

df_all_2 = net_rt[~net_rt.target.str.contains("#eleicoes2020")]



df_all_2

df_all_2 = df_all_2.reset_index(drop=True)

R = nx.from_pandas_edgelist(df_all_2, 'source', 'target', ['weight'])

for component in list(nx.connected_components(R)):
    if len(component)<10:
        for node in component:
            R.remove_node(node)

d = dict(R.degree(weight='weight'))

pos = nx.spring_layout(R)
edges = R.edges()

for node in R.nodes:
    R.nodes[node]['pos'] = list(pos[node])

edge_x = []
edge_y = []
for edge in R.edges():
    x0, y0 = R.nodes[edge[0]] ['pos']
    x1, y1 = R.nodes[edge[1]] ['pos']
    edge_x.append(x0)
    edge_x.append(x1)
    edge_x.append(None)
    edge_y.append(y0)
    edge_y.append(y1)
    edge_y.append(None)

edge_trace = go.Scatter(
    x=edge_x, y=edge_y,
    line=dict(width=0.5, color='#888'),
    hoverinfo='text+x+y',
    mode='lines')

node_x = []
node_y = []
for node in R.nodes():
    x, y = R.nodes[node] ['pos']
    node_x.append(x)
    node_y.append(y)

node_trace = go.Scatter(
    x=node_x, y=node_y,
    mode='markers',
    hoverinfo='text',
    marker=dict(
        showscale=True,
        # colorscale options
        #'Greys' | 'YlGnBu' | 'Greens' | 'YlOrRd' | 'Bluered' | 'RdBu' |
        #'Reds' | 'Blues' | 'Picnic' | 'Rainbow' | 'Portland' | 'Jet' |
        #'Hot' | 'Blackbody' | 'Earth' | 'Electric' | 'Viridis' |
        colorscale='Viridis',
        reversescale=True,
        color=[],
        size=10,
        colorbar=dict(
            thickness=15,
            title='Centralidade de grau (ponderada)',
            xanchor='left',
            titleside='right'
        ),
        line_width=2))

node_adjacencies = []
node_text = [f'<br>label: {sl}' for n, sl in zip(R.nodes, R)]
for node, adjacencies in enumerate(R.adjacency()):
    node_adjacencies.append(len(adjacencies[1]))
    #node_text.append('# of connections: '+str(len(adjacencies[1])))

node_trace.marker.color =  [v/200 for v in d.values()]
node_trace.text = node_text
#node_trace.marker.size = node_adjacencies [i * 5 for i in node_adjacencies]
node_trace.marker.size = [v/200 for v in d.values()] #[i * 5 for i in node_adjacencies]

fig19 = go.Figure(data=[edge_trace, node_trace],
             layout=go.Layout(
                title='<br>Rede de Retweets',
                titlefont_size=16,
                showlegend=False,
                hovermode='closest',
                margin=dict(b=20,l=8,r=8,t=40),
                width=750, height=600,
                xaxis=dict(showgrid=False, zeroline=False, showticklabels=False),
                yaxis=dict(showgrid=False, zeroline=False, showticklabels=False))
                )
fig19.show()

print(nx.info(R))

groups_fig20 =  collections.Counter(d).most_common(25)
groups_fig20 = pd.DataFrame.from_dict(groups_fig20,  orient='columns')
groups_fig20.columns = ['hashtag ou indivíduo', 'centralidade']

table_groups_fig20 = go.Figure(data=[go.Table(
    header=dict(values=list(groups_fig20.columns),
                fill_color='paleturquoise',
                align='left'),
    cells=dict(values=groups_fig20.transpose().values.tolist(),
               fill_color='lavender',
               align='left'))
])
table_groups_fig20.update_layout(margin=dict(l=10, r=10, t=10, b=10),  paper_bgcolor="LightSteelBlue")
table_groups_fig20.show()

#degree_hist_R = nx.degree_histogram(R)  
#fig = px.histogram(degree_hist_R)
#fig.update_layout(showlegend=False)

net_all = pd.concat([net1, net2.reindex(net1.index)], axis=1)
net_all = net_all.melt(id_vars="Nome_do_usuário")
net_all['value'] = net_all['value'].str.replace('[','').str.replace(']','').str.replace(',','')
net_all['value'].replace('', np.nan, inplace=True)
net_all.dropna(inplace=True)
net_all.drop(['variable'], axis=1, inplace=True)
net_all["count"] = 1
net_all = net_all.groupby(["Nome_do_usuário", "value"])["count"].count().reset_index()

flattened_hashtags_df = pd.DataFrame(net_all['value'])

popular_hashtags = flattened_hashtags_df.groupby('value').size()\
                                        .reset_index(name='counts')\
                                        .sort_values('counts', ascending=False)\
                                        .reset_index(drop=True)

# take hashtags which appear at least this amount of times
min_appearance = 5
# find popular hashtags - make into python set for efficiency
popular_hashtags_set = set(popular_hashtags[
                           popular_hashtags.counts>=min_appearance
                           ]['value'])



# make a new column with only the popular hashtags
popular_hashtags = pd.DataFrame(popular_hashtags_set, columns=['popular_hashtags'])

popular_hashtags = popular_hashtags.replace('onaro','#bolsonaro')

popular_hashtags

# make new dataframe
hashtag_vector_df = popular_hashtags.loc[:, ['popular_hashtags']]

for hashtag in popular_hashtags_set:
    # make columns to encode presence of hashtags
    hashtag_vector_df['{}'.format(hashtag)] = hashtag_vector_df.popular_hashtags.apply(
        lambda hashtag_list: int(hashtag in hashtag_list))

hashtag_matrix = hashtag_vector_df.set_index('popular_hashtags')

hash = nx.from_pandas_adjacency(hashtag_matrix)
hash.remove_edges_from(nx.selfloop_edges(hash))

for component in list(nx.connected_components(hash)):
    if len(component)<5:
        for node in component:
            hash.remove_node(node)

d = dict(hash.degree(weight='weight'))

pos = nx.fruchterman_reingold_layout(hash)
edges = hash.edges()

#%%capture
#!pip install python-louvain
#from community import community_louvain

partition = community_louvain.best_partition(hash, weight='weight')

colors = [partition[n] for n in hash.nodes()]

for node in hash.nodes:
    hash.nodes[node]['pos'] = list(pos[node])

edge_x = []
edge_y = []
for edge in hash.edges():
    x0, y0 = hash.nodes[edge[0]] ['pos']
    x1, y1 = hash.nodes[edge[1]] ['pos']
    edge_x.append(x0)
    edge_x.append(x1)
    edge_x.append(None)
    edge_y.append(y0)
    edge_y.append(y1)
    edge_y.append(None)

edge_trace = go.Scatter(
    x=edge_x, y=edge_y,
    line=dict(width=0.5, color='#888'),
    hoverinfo='text+x+y',
    mode='lines')

node_x = []
node_y = []
for node in hash.nodes():
    x, y = hash.nodes[node] ['pos']
    node_x.append(x)
    node_y.append(y)

node_trace = go.Scatter(
    x=node_x, y=node_y,
    mode='markers',
    hoverinfo='text',
    marker=dict(
        showscale=True,
        # colorscale options
        #'Greys' | 'YlGnBu' | 'Greens' | 'YlOrRd' | 'Bluered' | 'RdBu' |
        #'Reds' | 'Blues' | 'Picnic' | 'Rainbow' | 'Portland' | 'Jet' |
        #'Hot' | 'Blackbody' | 'Earth' | 'Electric' | 'Viridis' |
        colorscale= 'Rainbow',
        reversescale=True,
        color=[],
        size=10,
        colorbar=dict(
            thickness=15,
            title='Comunidades',
            xanchor='left',
            titleside='right'
        ),
        line_width=2))

node_adjacencies = []
node_text = [f'<br>label: {sl}' for n, sl in zip(hash.nodes, hash)]
for node, adjacencies in enumerate(hash.adjacency()):
    node_adjacencies.append(len(adjacencies[1]))
    #node_text.append('# of connections: '+str(len(adjacencies[1])))

node_trace.marker.color = colors #[v for v in d.values()] # node_adjacencies
node_trace.text = node_text
#node_trace.marker.size = node_adjacencies [i * 5 for i in node_adjacencies]
node_trace.marker.size = [v*4 for v in d.values()] #[i * 5 for i in node_adjacencies]

fig21 = go.Figure(data=[edge_trace, node_trace],
             layout=go.Layout(
                title='<br>Rede de Hashtags',
                titlefont_size=16,
                showlegend=False,
                hovermode='closest',
                margin=dict(b=20,l=5,r=5,t=40),
                width=750, height=600,
                xaxis=dict(showgrid=False, zeroline=False, showticklabels=False),
                yaxis=dict(showgrid=False, zeroline=False, showticklabels=False))
                )
fig21.show()

groups_fig22 =  collections.Counter(d).most_common(25)
groups_fig22 = pd.DataFrame.from_dict(groups_fig22,  orient='columns')
groups_fig22.columns = ['hashtags', 'centralidade']

table_groups_fig22 = go.Figure(data=[go.Table(
    header=dict(values=list(groups_fig22.columns),
                fill_color='paleturquoise',
                align='left'),
    cells=dict(values=groups_fig22.transpose().values.tolist(),
               fill_color='lavender',
               align='left'))
])
table_groups_fig22.update_layout(margin=dict(l=10, r=10, t=10, b=10),  paper_bgcolor="LightSteelBlue")
table_groups_fig22.show()

print(nx.info(hash))

degree_hist_hash = nx.degree_histogram(hash)  
fig = px.histogram(degree_hist_hash)
fig.update_layout(showlegend=False)

df_all = net_all
df_all.columns = ['source', 'target', 'weight']
#df_all = df_all[df['weight'] != 0]  # remove non-connected nodes

df_all_2 = net_all[~df_all.target.str.contains("#eleicoes2020")]

df_all_2 = df_all_2.reset_index(drop=True)

df_all_2 = df_all_2[df_all_2['weight'] >2]

net1 = pd.DataFrame(twitter_dt['Nome_do_usuário'])

net3 = pd.DataFrame(twitter_dt['Menções_de_usuários'].str.split().values.tolist())

net_mention = pd.concat([net1, net3.reindex(net1.index)], axis=1)
net_mention = net_mention.melt(id_vars="Nome_do_usuário")

net_mention['value'] = net_mention['value'].str.replace('[','').str.replace(']','').str.replace(',','')
net_mention['value'].replace('', np.nan, inplace=True)
net_mention.dropna(inplace=True)

net_mention.drop(['variable'], axis=1, inplace=True)

net_mention["count"] = 1
net_mention = net_mention.groupby(["Nome_do_usuário", "value"])["count"].count().reset_index()

df_mention_all = net_mention
df_mention_all.columns = ['source', 'target', 'weight']
#df_all = df_all[df['weight'] != 0]  # remove non-connected nodes

df_mention_all.shape

M = nx.from_pandas_edgelist(df_mention_all, 'source', 'target', ['weight'])
M.remove_edges_from(nx.selfloop_edges(M))

for component in list(nx.connected_components(M)):
    if len(component)<30:
        for node in component:
            M.remove_node(node)

#fig = figure(figsize=(10, 10))
pos = nx.spring_layout(M)
edges = M.edges()
#weights = [G[u][v]['weight'] for u,v in edges]
#nx.draw(G, pos, linewidths = 0.2, width = [i for i in weights if i<5], node_size = 85, alpha=0.5, node_color="indigo", with_labels=False, font_size=10)

d = dict(M.degree(weight='weight'))

for node in M.nodes:
    M.nodes[node]['pos'] = list(pos[node])

edge_x = []
edge_y = []
for edge in M.edges():
    x0, y0 = M.nodes[edge[0]] ['pos']
    x1, y1 = M.nodes[edge[1]] ['pos']
    edge_x.append(x0)
    edge_x.append(x1)
    edge_x.append(None)
    edge_y.append(y0)
    edge_y.append(y1)
    edge_y.append(None)

edge_trace = go.Scatter(
    x=edge_x, y=edge_y,
    line=dict(width=0.5, color='#888'),
    hoverinfo='text',
    mode='lines')

node_x = []
node_y = []
for node in M.nodes():
    x, y = M.nodes[node] ['pos']
    node_x.append(x)
    node_y.append(y)

node_trace = go.Scatter(
    x=node_x, y=node_y,
    mode='markers',
    hoverinfo='text',
    marker=dict(
        showscale=True,
        # colorscale options
        #'Greys' | 'YlGnBu' | 'Greens' | 'YlOrRd' | 'Bluered' | 'RdBu' |
        #'Reds' | 'Blues' | 'Picnic' | 'Rainbow' | 'Portland' | 'Jet' |
        #'Hot' | 'Blackbody' | 'Earth' | 'Electric' | 'Viridis' |
        colorscale='YlGnBu',
        reversescale=True,
        color=[],
        size=5,
        colorbar=dict(
            thickness=10,
            title='Centralidade de grau (ponderada)',
            xanchor='left',
            titleside='right'
        ),
        line_width=2))

node_adjacencies = []
node_text = [f'<br>label: {sl}' for n, sl in zip(M.nodes, M)]
for node, adjacencies in enumerate(M.adjacency()):
    node_adjacencies.append(len(adjacencies[1]))
    node_text.append('# of connections: '+str(len(adjacencies[1])))

node_trace.marker.color = [v/50 for v in d.values()] #node_adjacencies
node_trace.text = node_text
node_trace.marker.size = [v/50 for v in d.values()]  #[element * 1.5 for element in node_adjacencies]

fig23 = go.Figure(data=[edge_trace, node_trace],
             layout=go.Layout(
                title='<br>Rede de menções de indivíduos versus indivíduos',
                titlefont_size=14,
                showlegend=False,
                hovermode='closest',
                margin=dict(b=20,l=5,r=5,t=40),
                width=750, height=600,
                xaxis=dict(showgrid=False, zeroline=False, showticklabels=False),
                yaxis=dict(showgrid=False, zeroline=False, showticklabels=False))
                )
fig23.show()

groups_fig24 =  collections.Counter(d).most_common(25)
groups_fig24 = pd.DataFrame.from_dict(groups_fig24,  orient='columns')
groups_fig24.columns = ['Menções ou indivíduos', 'centralidade']

table_groups_fig24 = go.Figure(data=[go.Table(
    header=dict(values=list(groups_fig24.columns),
                fill_color='paleturquoise',
                align='left'),
    cells=dict(values=groups_fig24.transpose().values.tolist(),
               fill_color='lavender',
               align='left'))
])
table_groups_fig24.update_layout(margin=dict(l=10, r=10, t=10, b=10),  paper_bgcolor="LightSteelBlue")
table_groups_fig24.show()

print(nx.info(M))

go.table

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# !pip install teneto 
# from teneto import TemporalNetwork

net_temp  = pd.concat([twitter_dt['Nome_do_usuário'], twitter_dt['Tempo']], axis = 1)

net_temp

net_temp = pd.concat([net_temp, net2.reindex(net1.index)], axis=1)

net_temp

net_temp = net_temp.melt(id_vars=['Nome_do_usuário', 'Tempo'])

net_temp

net_temp['value'] = net_temp['value'].str.replace('[','').str.replace(']','').str.replace(',','')
net_temp['value'].replace('', np.nan, inplace=True)
net_temp.dropna(inplace=True)

net_temp.drop(['variable'], axis=1, inplace=True)

net_temp

net_temp["count"] = 1
net_temp = net_temp.groupby(["Nome_do_usuário", "Tempo", "value"])["count"].count().reset_index()

net_temp

temp_hashtags = net_temp.groupby(['Tempo', 'value']).size()\
                                        .reset_index(name='counts')\
                                        .sort_values('counts', ascending=False)\
                                        .reset_index(drop=True)

temp_hashtags

temp_hashtags['Tempo'] = temp_hashtags.Tempo.astype('datetime64[ns]').dt.to_period('1d')

temp_hashtags

temp_hashtags = temp_hashtags.drop(columns='counts')

temp_hashtags = temp_hashtags.drop_duplicates()

temp_hashtags

temp_hashtags = pd.DataFrame(temp_hashtags.loc[temp_hashtags['value'].isin(['#eleicoesmunicipais2020','#politica', '#elections2020', '#vereador', '#covid19', '#eleicoesmunicipais', '#forabolsoanro'])])
temp_hashtags = temp_hashtags.reset_index()

net_all['target'].value_counts().nlargest(15)

# = temp_hashtags[['value', 'Tempo', 'counts']]

#temp_hashtags_n = temp_hashtags_n.rename(columns={"value": "i", "Tempo": "t",  "counts": "w"})

temp_hashtags

type(temp_hashtags)

tlabs = ['2014','2015','2016','2017','2018','2019','2020','2021','2022','2023']
#nlabs = temp_hashtags['value'][0:10]
nlabs = ['1','2','3','4','5','6','7','8','9','10']

tlabs = temp_hashtags['Tempo'][0:20].to_list()
nlabs = temp_hashtags['value'][0:20].to_list()

TemporalNetwork

tlabs = ['2014','2015','2016','2017','2018']
tunit = 'years'
nlabs = ['Ashley', 'Blake', 'Casey']
tnet = TemporalNetwork(nodelabels=nlabs, timeunit=tunit, timelabels=tlabs, nettype='bu')
tnet.generatenetwork('rand_binomial',size=(3,5), prob=0.5)
tnet.plot('slice_plot', cmap='Set2')
plt.show()

from io import BytesIO
import base64

twitter_dt.head()

import sys
print(sys.version)

"""## Dash"""

# the style arguments for the main content page.
CONTENT_STYLE = {
    'margin-left': '5%',
    'margin-right': '5%',
    'padding': '20px 10p'
}

TEXT_STYLE = {
    'textAlign': 'center',
    'color': '#191970'
}

content_mark = dcc.Markdown('''

O Dataset utilizado neste projeto foi coletado através da [API do Twitter](http://commonmark.org/help) e da library [Tweepy](https://www.tweepy.org/). O dataset está disponível no repositório do projeto no meu [GitHub](https://github.com/Fmarcelneves/Twitter_Network_Analytics). 

A coleta de dados durou cerca de um mês, referente as eleições municipais do Brasil de 2020, 
gerando cerca de **120 mil tweets**. 

### Análise exploratória 

''')

content_first_row = dbc.Row(
    [
        dbc.Col(
            dcc.Graph(figure=fig1), md=4
        ),
        dbc.Col(
            dcc.Graph(figure=fig2), md=4
        ),
        dbc.Col(
            dcc.Graph(figure=fig3), md=4
        )
    ]
)

content_second_row = dbc.Row(
    [
        dbc.Col(
            dcc.Graph(figure=fig4.update_layout(showlegend=False)), md=4
        ),
        dbc.Col(
            dcc.Graph(figure=fig5.update_layout(showlegend=False)), md=4
        ),
        dbc.Col(
            dcc.Graph(figure=fig6.update_layout(showlegend=False)), md=4
        )
    ]
)

content_third_row = dbc.Row(
    [
        dbc.Col(
            dcc.Graph(figure=fig7), md=12
        )
    ]
)

content_fourth_row = dbc.Row(
    [
        dbc.Col(
            dcc.Graph(figure=fig8), md=12
        )
    ]
)

content_fifth_row = dbc.Row(
    [
        dbc.Col(
            dcc.Graph(figure=fig9), md=12
        )
    ]
)

content_sixth_row = dbc.Row(
    [
        dbc.Col(
            dcc.Graph(figure=fig10), md=12
        )
    ]
)


content_seventh_row = dbc.Row(
    [
        dbc.Col(
            dcc.Graph(figure=fig11), md=6
        ), 
        dbc.Col(
            dcc.Graph(figure=fig12), md=6
        )
    ]
)

content_eighth_row = dbc.Row(
    [
        dbc.Col(
            dcc.Graph(figure=fig13), md=12
        )
    ]
)

content_ninth_row = dbc.Row(
    [
        dbc.Col(
            dcc.Graph(figure=fig14), md=12
        )
    ]
)

content_mark_2 = dcc.Markdown('''

### Análise de sentimentos 

Para a análise de sentimentos dos tweets (neutro, positivo e negativo), utilizamos o modelo de regressão logística multinomial. 

''')

content_tenth_row = dbc.Row(
     [
        dbc.Col(
            dcc.Graph(figure=fig15), md=12
        )
    ]
)

content_eleventh_row = dbc.Row(
   [
        dbc.Col(
            dcc.Graph(figure=fig16), md=6
        ),
        dbc.Col(
            dcc.Graph(figure=fig17), md=6
        )
    ]
)

content_twelfth_row = dbc.Row(
     [
        dbc.Col(
            dcc.Graph(figure=fig18), md=12
        )
    ]
)


content_mark_3 = dcc.Markdown('''

### Redes complexas 

Utilizamos redes para representar as interações entre individuos e hashtags, entre as hashtags e os individuos e suas menções.
Para compreender estas relações foram consideradas medidas de grau de centralidade (ponderadas) e de comunidades (Louvain)

''')


content_thirteenth_row = dbc.Row(
   [
        dbc.Col(
            dcc.Graph(figure=fig19), md=9
        ),
        dbc.Col(
            dcc.Graph(figure=table_groups_fig20), md=3
        )
    ]
)

content_fourteenth_row = dbc.Row(
   [
        dbc.Col(
            dcc.Graph(figure=fig21), md=9
        ),
        dbc.Col(
            dcc.Graph(figure=table_groups_fig22), md=3
        )
    ]
)

content_fifteenth_row = dbc.Row(
   [
        dbc.Col(
            dcc.Graph(figure=fig23), md=9
        ),
        dbc.Col(
            dcc.Graph(figure=table_groups_fig24), md=3
        )
    ]
)

content = html.Div(
    [
        html.H2('Twitter Network Analytics', style=TEXT_STYLE),
        html.Div(children='''
         Um framework para análise de dados de redes sociais usando análises e medidas de redes complexas, além de análise de sentimentos.
     '''),
        html.Hr(),
        content_mark,
        content_first_row,
        content_second_row,
        content_third_row,
        content_fourth_row,
        content_fifth_row,
        content_sixth_row,
        content_seventh_row,
        content_eighth_row,
        content_ninth_row,
        content_mark_2,
        content_tenth_row,
        content_eleventh_row,
        content_twelfth_row,
        content_mark_3,
        content_thirteenth_row,
        content_fourteenth_row,
        content_fifteenth_row, 
    ],
    style=CONTENT_STYLE
)

#app = dash.Dash(external_stylesheets=[dbc.themes.BOOTSTRAP])
app = JupyterDash(external_stylesheets=[dbc.themes.BOOTSTRAP])
app.layout = html.Div([content])

if __name__ == '__main__':
    app.run_server(debug=True, port=8051)